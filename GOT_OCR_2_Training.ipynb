{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training Code to be used"
      ],
      "metadata": {
        "id": "b4Wl-PW4ZCE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install verovio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3wArv--VRtX",
        "outputId": "5aeed7df-19c5-447b-d68d-bb09414c75bc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: verovio in /usr/local/lib/python3.12/dist-packages (5.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDZY2YM_LVyH",
        "outputId": "bee62bd1-2b3e-46e6-c81f-1f0900d20227"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcXYmaUmMcx5",
        "outputId": "04e382d9-2fae-4414-d183-76fb0da2a6be"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.12/dist-packages (from jiwer) (3.14.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from transformers import AutoModelForImageTextToText, AutoTokenizer, AutoProcessor, Trainer, TrainingArguments, TrainerCallback\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import cv2\n",
        "import evaluate\n",
        "import json\n",
        "import numpy as np\n",
        "from jiwer import cer as cer_jiwer"
      ],
      "metadata": {
        "id": "6w7Ntfb_ZIet"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    def in_colab():\n",
        "        try:\n",
        "            import google.colab\n",
        "            return True\n",
        "        except ImportError:\n",
        "            return False\n",
        "\n",
        "    if in_colab():\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        OCR_PATH = '/content/drive/MyDrive/OCR'\n",
        "    else:\n",
        "        OCR_PATH = '.'"
      ],
      "metadata": {
        "id": "skELlB7iaKb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc3c586-91ec-4db2-eabf-b49f8b00ab44"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"stepfun-ai/GOT-OCR-2.0-hf\"\n",
        "OUTPUT_DIR = \"./got_ocr_finetuned\"\n",
        "\n",
        "MAX_LENGTH = 2048\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "GRAD_ACCUMULATION = 1\n",
        "LEARNING_RATE = 15 * 1e-5"
      ],
      "metadata": {
        "id": "GtypI8LUZKid"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, use_fast=True)\n",
        "\n",
        "tokenizer = processor.tokenizer\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "image_token_id = model.config.image_token_index\n",
        "num_image_tokens = model.config.image_seq_length"
      ],
      "metadata": {
        "id": "QeHNXNmkZYJu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.enable_input_require_grads()\n",
        "model.print_trainable_parameters()\n",
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "eIRZuizNZcLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dfee442-4b4c-4572-d353-100beef02696"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,572,864 || all params: 562,101,504 || trainable%: 0.2798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GOTOCRDataset(Dataset):\n",
        "    def __init__(self, data_list, processor, max_length=2048):\n",
        "        self.data = data_list\n",
        "        self.processor = processor\n",
        "        self.tokenizer = processor.tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image_path = os.path.join(OCR_PATH, \"Samples\", item[\"image_path\"])\n",
        "        gt = item[\"ground_truth\"]\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        base = self.processor(image, return_tensors=\"pt\")\n",
        "        base_input_ids = base[\"input_ids\"][0]\n",
        "        pixel_values = base[\"pixel_values\"][0]\n",
        "\n",
        "        answer_text = gt + self.tokenizer.eos_token\n",
        "\n",
        "        answer_ids = self.tokenizer(\n",
        "            answer_text,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=False,\n",
        "        ).input_ids[0]\n",
        "\n",
        "        full_input_ids = torch.cat([base_input_ids, answer_ids], dim=0)\n",
        "        attention_mask = torch.ones_like(full_input_ids)\n",
        "\n",
        "        assert base_input_ids.shape[0] < self.max_length, \"max_length too small; would cut image tokens\"\n",
        "        if full_input_ids.shape[0] > self.max_length:\n",
        "            full_input_ids = full_input_ids[: self.max_length]\n",
        "            attention_mask = attention_mask[: self.max_length]\n",
        "\n",
        "        labels = full_input_ids.clone()\n",
        "        labels[: base_input_ids.shape[0]] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": full_input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"labels\": labels,\n",
        "        }"
      ],
      "metadata": {
        "id": "_UnSXVMtZuYO"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = [\n",
        "    {'image_path': 'OCR_Train_1.png', 'ground_truth': 'Abog. Jorge Bresanovich Musa\\nActuario Judicial'},\n",
        "    {'image_path': 'OCR_Train_2.png', 'ground_truth': 'Dr. Miguel A. Rodas Ruiz Díaz\\nMiembro del Tribunal de Apelación\\nCivil y Comercial - 4ta. Sala'},\n",
        "    {'image_path': 'OCR_Train_3.png', 'ground_truth': 'Firmado digitalmente por: NIDIA\\nLETIZIA PAREDES ARIAS (JUEZ/A)'},\n",
        "    {'image_path': 'OCR_Train_4.png', 'ground_truth': 'BIRIANA BENÍTEZ FARIA\\nMiembro\\nTribunal de Apelación 2a. Sala Penal'},\n",
        "    {'image_path': 'OCR_Train_5.png', 'ground_truth': 'Dr. Eugenio Jiménez R\\nMinistro'},\n",
        "    {'image_path': 'OCR_Train_6.png', 'ground_truth': 'Dra. Miryam Peña Candia\\nMinistra'},\n",
        "    {'image_path': 'OCR_Train_7.png', 'ground_truth': 'Abg. Piedad Ozuna Wood Secretaria\\nJudicial - C.S.J.'},\n",
        "    {'image_path': 'OCR_Train_8.png', 'ground_truth': 'Luis María Benítez Riera\\nMinistro'},\n",
        "    {'image_path': 'OCR_Train_9.png', 'ground_truth': 'ELIO RUBEN OVELAR FRUTOS\\nJUEZ PENAL DE SENTENCIA'},\n",
        "    {'image_path': 'OCR_Train_10.png', 'ground_truth': 'ENRIQUE MONGELÓS AQUINO\\nMiembro del Tribunal de Apelación\\nCivil y Comercial - 4ta. Sala'},\n",
        "    {'image_path': 'OCR_Train_11.png', 'ground_truth': 'DR. JOSE WALDR SERVÍN\\nMiembro del Tribunal Apelación\\nPenal, 3ª Sala. Capital'},\n",
        "    {'image_path': 'OCR_Train_12.png', 'ground_truth': 'Luis María Benítez Riera\\nMinistro'},\n",
        "    {'image_path': 'OCR_Train_13.png', 'ground_truth': 'Abg. Gabriela Mora\\nActuaria Judicial'},\n",
        "    {'image_path': 'OCR_Train_14.png', 'ground_truth': 'Abog. Carol Fernández Cáceres\\nActuaria Judicial'},\n",
        "    {'image_path': 'OCR_Train_15.png', 'ground_truth': 'Jorge L. Barreto Alfonso\\nActuario Judicial'},\n",
        "    {'image_path': 'OCR_Train_16.png', 'ground_truth': 'Dr. Edward Vittone\\nMiembro\\nTribunal de Cuentas'},\n",
        "    {'image_path': 'OCR_Train_17.png', 'ground_truth': 'Dr. Linneo Ynsfrán Saldívar\\nMiembro 5ta. Sala'},\n",
        "    {'image_path': 'OCR_Train_18.png', 'ground_truth': 'Dr. Manuel Dejesús Ramírez Candia\\nMINISTRO'},\n",
        "    {'image_path': 'OCR_Train_19.png', 'ground_truth': 'Abog. Karina Penoni\\nSecretaria'},\n",
        "    {'image_path': 'OCR_Train_20.png', 'ground_truth': 'Dr. MANUEL AGUIRRE RODAS\\nJUEZ PENAL'},\n",
        "    {'image_path': 'OCR_Train_21.png', 'ground_truth': 'Abg. Lourdes Nathalia Martínez\\nActuaria Judicial'},\n",
        "    {'image_path': 'OCR_Train_22.png', 'ground_truth': 'Arsenio Coronel Benítez\\nPresidente\\nTribunal de Cuentas'},\n",
        "    {'image_path': 'OCR_Train_23.png', 'ground_truth': 'DR. RODRIGO A. ESCOBAR E.\\nMiembro del Tribunal de Cuentas\\nPrimera Sala'},\n",
        "    {'image_path': 'OCR_Train_24.png', 'ground_truth': 'TERCERA SALA\\nTRIBUNAL DE APELACION PENAL'},\n",
        "    {'image_path': 'OCR_Train_25.png', 'ground_truth': 'Abg. Ma. Estela Bóveda Curril\\nActuaria Judicial'},\n",
        "    {'image_path': 'OCR_Train_26.png', 'ground_truth': 'Abog. Benita Duarte Olmedo\\nActuaria Judicial'},\n",
        "    {'image_path': 'OCR_Train_27.png', 'ground_truth': 'WILFRIDO PERALTA\\nJUEZ PENAL'},\n",
        "    {'image_path': 'OCR_Train_28.png', 'ground_truth': 'Dr. GIUSEPPE FOSSATI LÓPEZ\\nMiembro del Tribunal de Apelación\\nCivil y Comercial de la Capital\\nCuarta Sala'},\n",
        "    {'image_path': 'OCR_Train_29.png', 'ground_truth': 'María Luz Martínez Vázquez\\nJuez'},\n",
        "    {'image_path': 'OCR_Train_30.png', 'ground_truth': 'César Antonio Garay'},\n",
        "    {'image_path': 'OCR_Train_31.png', 'ground_truth': 'Prof. Dra. Ma. Carolina Llanes O.\\nMinistra'},\n",
        "    {'image_path': 'OCR_Train_32.png', 'ground_truth': 'Dr. GIUSEPPE FOSSATI LÓPEZ\\nMiembro del Tribunal de Apelación\\nCivil y Comercial de la Capital\\nCuarta Sala'},\n",
        "    {'image_path': 'OCR_Train_33.png', 'ground_truth': 'Abg. LOURDES E. PEÑA\\nJueza Penal\\nLiq. y Sent. Nº 1'},\n",
        "    {'image_path': 'OCR_Train_34.png', 'ground_truth': 'Abg. Wilfrido Méndez\\nActuario Judicial'},\n",
        "    {'image_path': 'OCR_Train_35.png', 'ground_truth': 'Abg. VICTOR HUGO ALFIERI OURIA\\nJuez Penal'},\n",
        "    {'image_path': 'OCR_Train_36.png', 'ground_truth': 'Abg. Lourdes Nathalia Martinez\\nActuaria Judicial'},\n",
        "    {'image_path': 'OCR_Train_37.png', 'ground_truth': 'Abog. Mercedes Sosa G.\\nActuaria Judicial'},\n",
        "    {'image_path': 'OCR_Train_38.png', 'ground_truth': 'Abog. Darío Báez Ferreira\\nJuez Penal'},\n",
        "    {'image_path': 'OCR_Train_39.png', 'ground_truth': 'VICTOR MANUEL MEDINA S.\\nJUEZ'},\n",
        "    {'image_path': 'OCR_Train_40.png', 'ground_truth': 'Abg. Adriana Sánchez Schlunk\\nActuaria Judicial'},\n",
        "    {'image_path': 'OCR_Train_41.png', 'ground_truth': 'PODER JUDICIAL\\nJUZGADO PENAL\\nDE SENTENCIA\\nNº 22\\nAsunción - Paraguay'},\n",
        "    {'image_path': 'OCR_Train_42.png', 'ground_truth': 'Poder Judicial\\nCámara de Apelación\\nEn lo Civil y Comercial - 1ª Sala'},\n",
        "    {'image_path': 'OCR_Train_43.png', 'ground_truth': 'Poder Judicial\\nSección Estadística Penal\\nCapital'},\n",
        "    {'image_path': 'OCR_Train_44.png', 'ground_truth': 'GERALDINE CASBS M.\\nMiembro Trib. Apelación Laboral'},\n",
        "    {'image_path': 'OCR_Train_45.png', 'ground_truth': 'Abg. Nancy Aquino\\nActuaria Judicial'},\n",
        "    {'image_path': 'OCR_Train_46.png', 'ground_truth': 'PODER JUDICIAL\\nREPÚBLICA DEL PARAGUAY\\n2ª SALA\\nTRIBUNAL DE CUENTAS'},\n",
        "    {'image_path': 'OCR_Train_47.png', 'ground_truth': 'Abog. SILVIA SANABRIA\\nActuaria Judicial\\nCoordinación de Juicios Orales'},\n",
        "    {'image_path': 'OCR_Train_48.png', 'ground_truth': 'Mónica Ramona Reguera\\nActuaria Judicial\\nTribunal de Apelación Civil y\\nComercial Cuarta Sala'},\n",
        "    {'image_path': 'OCR_Train_49.png', 'ground_truth': 'Dr. Víctor Ríos Ojeda\\nMinistro'},\n",
        "    {'image_path': 'OCR_Train_50.png', 'ground_truth': 'Dr. A. Martin Avalos Valdez\\nMiembro-T Contencioso-Administrativo\\n1ra Sala'},\n",
        "]\n",
        "\n",
        "val_data = [\n",
        "    {'image_path': 'Test1.png', 'ground_truth': 'Dr. Mario Y. Maldana Griffith\\nMiembro'},\n",
        "    {'image_path': 'Test2.png', 'ground_truth': 'Abog. Carol Fernández Cáceres\\nActuaria Judicial'},\n",
        "    {'image_path': 'Test3.png', 'ground_truth': 'Dr. AGUSTIN LOVERA CAÑETE\\nMIEMBRO'},\n",
        "    {'image_path': 'Test4.png', 'ground_truth': 'og. SANDRA FARÍAS de FERNÁNDEZ\\nJuez Penal de Sentencia\\nNo. 3'},\n",
        "    {'image_path': 'Test5.png', 'ground_truth': 'Abg. Wilfrido Méndez\\nActuario Judicial'},\n",
        "    {'image_path': 'Test6.png', 'ground_truth': 'ALMA MENDEZ DE BUONGERMINI\\nMiembro Trib. Apel. Laboral'},\n",
        "    {'image_path': 'Test7.png', 'ground_truth': 'JULIO CÉSAR CENTENO B.\\nMiembro'},\n",
        "    {'image_path': 'Test8.png', 'ground_truth': 'Eulalia Villalba Ojeda\\nAsistente\\nEstadística Penal - Asuncion'},\n",
        "    {'image_path': 'Test9.png', 'ground_truth': 'ABG. LINA CASCO D.\\nActuaria Judicial'},\n",
        "    {'image_path': 'Test10.png', 'ground_truth': 'YOLANDA PORTILLO\\nJuez Penal'},\n",
        "]\n"
      ],
      "metadata": {
        "id": "51-z8fZU9r8i"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_data_from_jsonl(file_path):\n",
        "#     data = []\n",
        "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#         for line in f:\n",
        "#             if line.strip():  # skip empty lines\n",
        "#                 data.append(json.loads(line))\n",
        "#     return data\n",
        "\n",
        "# train_data = load_data_from_jsonl(os.path.join(OCR_PATH, \"train.jsonl\"))\n",
        "# val_data = load_data_from_jsonl(os.path.join(OCR_PATH,\"val.jsonl\"))"
      ],
      "metadata": {
        "id": "ruNoxa5r7TNV"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = GOTOCRDataset(\n",
        "    train_data,\n",
        "    processor,\n",
        "    max_length=MAX_LENGTH,\n",
        ")"
      ],
      "metadata": {
        "id": "epB6nVFGiTWW"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = GOTOCRDataset(\n",
        "    val_data,\n",
        "    processor,\n",
        "    max_length=MAX_LENGTH,\n",
        ")"
      ],
      "metadata": {
        "id": "gs1-tVA7KWdY"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    pad_id = processor.tokenizer.pad_token_id or processor.tokenizer.eos_token_id\n",
        "\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "        [b[\"input_ids\"] for b in batch],\n",
        "        batch_first=True,\n",
        "        padding_value=pad_id,\n",
        "    )\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(\n",
        "        [b[\"labels\"] for b in batch],\n",
        "        batch_first=True,\n",
        "        padding_value=-100,\n",
        "    )\n",
        "    attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
        "        [b[\"attention_mask\"] for b in batch],\n",
        "        batch_first=True,\n",
        "        padding_value=0,\n",
        "    )\n",
        "    pixel_values = torch.stack([b[\"pixel_values\"] for b in batch])\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"labels\": labels,\n",
        "    }"
      ],
      "metadata": {
        "id": "So92MDk8iTwV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cer_metric = evaluate.load(\"cer\")\n",
        "\n",
        "def _normalize_text(s: str) -> str:\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = s.replace(\"\\r\", \"\\n\")\n",
        "    lines = [ln.strip() for ln in s.splitlines()]\n",
        "    s = \"\\n\".join(lines)\n",
        "    s = \" \".join(s.split())\n",
        "    return s\n",
        "\n",
        "def evaluate_ocr_dataset(\n",
        "    model,\n",
        "    processor,\n",
        "    examples,\n",
        "    max_new_tokens: int = 40,\n",
        "    num_beams: int = 1,\n",
        "    print_samples: bool = False,\n",
        "):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    all_preds = []\n",
        "    all_refs = []\n",
        "\n",
        "    for ex in examples:\n",
        "        img_path = os.path.join(OCR_PATH, \"Samples\", ex[\"image_path\"])\n",
        "        ref_text = ex[\"ground_truth\"]\n",
        "\n",
        "        bgr = cv2.imread(img_path)\n",
        "        if bgr is None:\n",
        "            print(f\"Could not read image: {img_path}\")\n",
        "            continue\n",
        "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "        pil_image = Image.fromarray(rgb)\n",
        "\n",
        "        inputs = processor(pil_image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            gen_ids = model.generate(\n",
        "                **inputs,\n",
        "                do_sample=False,\n",
        "                num_beams=num_beams,\n",
        "                length_penalty=1,\n",
        "                tokenizer=processor.tokenizer,\n",
        "                stop_strings=\"<|im_end|>\",\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                no_repeat_ngram_size=4,\n",
        "                repetition_penalty=1.1,\n",
        "            )\n",
        "\n",
        "        prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "        gen_seq = gen_ids[0, prompt_len:]\n",
        "\n",
        "        pred_text = processor.decode(\n",
        "            gen_seq,\n",
        "            skip_special_tokens=True,\n",
        "        )\n",
        "\n",
        "        ref_norm = _normalize_text(ref_text)\n",
        "        pred_norm = _normalize_text(pred_text)\n",
        "\n",
        "        all_refs.append(ref_norm)\n",
        "        all_preds.append(pred_norm)\n",
        "\n",
        "        sample_cer = cer_jiwer(ref_norm, pred_norm)\n",
        "\n",
        "        if print_samples:\n",
        "            print(\"=\" * 80)\n",
        "            print(f\"IMAGE  : {img_path}\")\n",
        "            print(f\"REF    : {ref_norm}\")\n",
        "            print(f\"PRED   : {pred_norm}\")\n",
        "            print(f\"CER    : {sample_cer:.4f}\")\n",
        "\n",
        "    global_cer = cer_metric.compute(predictions=all_preds, references=all_refs)\n",
        "    exact = sum(p == r for p, r in zip(all_preds, all_refs)) / len(all_refs)\n",
        "\n",
        "    print(\"\\n\" + \"#\" * 80)\n",
        "    print(f\"GLOBAL CER      : {global_cer:.4f}\")\n",
        "    print(f\"EXACT MATCH RATE: {exact * 100:.2f}%\")\n",
        "    print(\"#\" * 80)\n",
        "\n",
        "    return {\"cer\": global_cer, \"exact_match\": exact}"
      ],
      "metadata": {
        "id": "iaBof6Hhx0cc"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CEREvaluationCallback(TrainerCallback):\n",
        "    def __init__(self, model, processor, eval_dataset, log_steps=False):\n",
        "        self.model = model\n",
        "        self.processor = processor\n",
        "        self.eval_dataset = eval_dataset\n",
        "        self.log_steps = log_steps\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        \"\"\"\n",
        "        Run evaluation at the end of every epoch.\n",
        "        \"\"\"\n",
        "        print(f\"\\n[Epoch {state.epoch:.0f}] Running CER Evaluation...\")\n",
        "\n",
        "        metrics = evaluate_ocr_dataset(\n",
        "            model=self.model,\n",
        "            processor=self.processor,\n",
        "            examples=self.eval_dataset,\n",
        "            max_new_tokens=30,\n",
        "            num_beams=3,\n",
        "            print_samples=False\n",
        "        )\n",
        "\n",
        "        self.model.train()"
      ],
      "metadata": {
        "id": "kKaC5SkhcgyG"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "Cer measured in Callback. Disable this if you are not preticularily concerned with the metric over epochs: result with ~0.10 cer is more consistent without it and it's faster."
      ],
      "metadata": {
        "id": "ni5y82--cXhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    num_train_epochs=10,\n",
        "    bf16=True,\n",
        "    fp16=False,\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "cer_callback = CEREvaluationCallback(\n",
        "    model=model,\n",
        "    processor=processor,\n",
        "    eval_dataset=val_data\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=collate_fn,\n",
        "    callbacks=[cer_callback],\n",
        ")\n",
        "\n",
        "# Loop is much faster without cer_callback.\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=dataset,\n",
        "#     data_collator=collate_fn,\n",
        "# )"
      ],
      "metadata": {
        "id": "CEcYMnX7Y2j6"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B2mAlccRzyYC",
        "outputId": "4acd155a-2538-4f43-bc96-85bfac7dbd4e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [130/130 06:49, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.982500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.209500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.076700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.692400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.686500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.296900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.505600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.280600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.353300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.215600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.166300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.140900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.082500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Epoch 1] Running CER Evaluation...\n",
            "\n",
            "################################################################################\n",
            "GLOBAL CER      : 0.2979\n",
            "EXACT MATCH RATE: 10.00%\n",
            "################################################################################\n",
            "\n",
            "[Epoch 2] Running CER Evaluation...\n",
            "\n",
            "################################################################################\n",
            "GLOBAL CER      : 0.2033\n",
            "EXACT MATCH RATE: 0.00%\n",
            "################################################################################\n",
            "\n",
            "[Epoch 3] Running CER Evaluation...\n",
            "\n",
            "################################################################################\n",
            "GLOBAL CER      : 0.2009\n",
            "EXACT MATCH RATE: 10.00%\n",
            "################################################################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Epoch 4] Running CER Evaluation...\n",
            "\n",
            "################################################################################\n",
            "GLOBAL CER      : 0.2530\n",
            "EXACT MATCH RATE: 10.00%\n",
            "################################################################################\n",
            "\n",
            "[Epoch 5] Running CER Evaluation...\n",
            "\n",
            "################################################################################\n",
            "GLOBAL CER      : 0.1655\n",
            "EXACT MATCH RATE: 10.00%\n",
            "################################################################################\n",
            "\n",
            "[Epoch 6] Running CER Evaluation...\n",
            "\n",
            "################################################################################\n",
            "GLOBAL CER      : 0.1726\n",
            "EXACT MATCH RATE: 0.00%\n",
            "################################################################################\n",
            "\n",
            "[Epoch 7] Running CER Evaluation...\n",
            "\n",
            "################################################################################\n",
            "GLOBAL CER      : 0.1087\n",
            "EXACT MATCH RATE: 0.00%\n",
            "################################################################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Epoch 8] Running CER Evaluation...\n",
            "\n",
            "################################################################################\n",
            "GLOBAL CER      : 0.0946\n",
            "EXACT MATCH RATE: 0.00%\n",
            "################################################################################\n",
            "\n",
            "[Epoch 9] Running CER Evaluation...\n",
            "\n",
            "################################################################################\n",
            "GLOBAL CER      : 0.0969\n",
            "EXACT MATCH RATE: 0.00%\n",
            "################################################################################\n",
            "\n",
            "[Epoch 10] Running CER Evaluation...\n",
            "\n",
            "################################################################################\n",
            "GLOBAL CER      : 0.1040\n",
            "EXACT MATCH RATE: 0.00%\n",
            "################################################################################\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=130, training_loss=1.5914938779977652, metrics={'train_runtime': 410.9062, 'train_samples_per_second': 1.217, 'train_steps_per_second': 0.316, 'total_flos': 381940830167040.0, 'train_loss': 1.5914938779977652, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = evaluate_ocr_dataset(\n",
        "    model,\n",
        "    processor,\n",
        "    val_data,\n",
        "    max_new_tokens=30,\n",
        "    num_beams=3,\n",
        "    print_samples=True,\n",
        ")\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pUNLVSLPmb8",
        "outputId": "f2e85bce-aed3-4d5b-cc3b-873207f11801"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test1.png\n",
            "REF    : Dr. Mario Y. Maldana Griffith Miembro\n",
            "PRED   : Dr. María Y. Maldana Griffith Metro Metro\n",
            "CER    : 0.2703\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test2.png\n",
            "REF    : Abog. Carol Fernández Cáceres Actuaria Judicial\n",
            "PRED   : Abog. Carol Fernández Cáceres Actuaria Judicial 21\n",
            "CER    : 0.0638\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test3.png\n",
            "REF    : Dr. AGUSTIN LOVERA CAÑETE MIEMBRO\n",
            "PRED   : DT. AGUSTIN LOVERA CANÉTE MIEMBRO\n",
            "CER    : 0.0909\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test4.png\n",
            "REF    : og. SANDRA FARÍAS de FERNÁNDEZ Juez Penal de Sentencia No. 3\n",
            "PRED   : ÓG. SÁNDRA FARIAS de FERNAN EZ Pena! de Setencia 1992 1\n",
            "CER    : 0.3000\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test5.png\n",
            "REF    : Abg. Wilfrido Méndez Actuario Judicial\n",
            "PRED   : Abg. Wilfrido Méndez Actuário Judicia\n",
            "CER    : 0.0526\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test6.png\n",
            "REF    : ALMA MENDEZ DE BUONGERMINI Miembro Trib. Apel. Laboral\n",
            "PRED   : ALMA MENDIZ DE GUONGERMINI Miembro Trib. Apel. Laboral\n",
            "CER    : 0.0370\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test7.png\n",
            "REF    : JULIO CÉSAR CENTENO B. Miembro\n",
            "PRED   : JULIO CESAR CENTENO B. Miembro\n",
            "CER    : 0.0333\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test8.png\n",
            "REF    : Eulalia Villalba Ojeda Asistente Estadística Penal - Asuncion\n",
            "PRED   : Ejulia Villalba Ojeda Asistente Estadística Penal - Asuncion\n",
            "CER    : 0.0492\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test9.png\n",
            "REF    : ABG. LINA CASCO D. Actuaria Judicial\n",
            "PRED   : ABG. LINA CASCOD. Actuaria Judicial\n",
            "CER    : 0.0278\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test10.png\n",
            "REF    : YOLANDA PORTILLO Juez Penal\n",
            "PRED   : XOLANDA PORTILLO Juez Penal\n",
            "CER    : 0.0370\n",
            "\n",
            "################################################################################\n",
            "GLOBAL CER      : 0.1040\n",
            "EXACT MATCH RATE: 0.00%\n",
            "################################################################################\n",
            "{'cer': 0.10401891252955082, 'exact_match': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Saving adapter and processor...\")\n",
        "model.save_pretrained(os.path.join(OCR_PATH, \"fine_tuned_model\"))\n",
        "processor.save_pretrained(os.path.join(OCR_PATH, \"fine_tuned_model\"))"
      ],
      "metadata": {
        "id": "gIrNJw-jaJsy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b5f759-262e-4ec7-ec88-0416562cd6c2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving adapter and processor...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference and Part of Evaluating"
      ],
      "metadata": {
        "id": "QYNKyU49_gDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
        "import torch\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from accelerate import Accelerator\n",
        "\n",
        "MODEL_ID = \"stepfun-ai/GOT-OCR-2.0-hf\"\n",
        "device = Accelerator().device\n",
        "\n",
        "# 1) Base model (no LoRA)\n",
        "base_model = AutoModelForImageTextToText.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=torch.float16\n",
        ").eval()\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "test_img = \"Test4.png\"\n",
        "image = cv2.imread(os.path.join(OCR_PATH, \"Samples\", test_img))\n",
        "pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "base_inputs = processor(pil_image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    base_ids = base_model.generate(\n",
        "        **base_inputs,\n",
        "        do_sample=False,\n",
        "        tokenizer=processor.tokenizer,\n",
        "        stop_strings=\"<|im_end|>\",\n",
        "        max_new_tokens=64,\n",
        "    )\n",
        "\n",
        "base_text = processor.decode(\n",
        "    base_ids[0, base_inputs[\"input_ids\"].shape[1]:],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "print(\"BASE:\\n\" + base_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKpkUUyKBrXG",
        "outputId": "eae7d629-7d21-4e7f-ebab-8d73babbaa81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASE:\n",
            "FERNAN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = evaluate_ocr_dataset(\n",
        "    model,\n",
        "    processor,\n",
        "    val_data,\n",
        "    max_new_tokens=30,\n",
        "    num_beams=3,\n",
        "    print_samples=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYA-jsffTmUa",
        "outputId": "eb1ce21b-ad3a-40bc-de58-691d8a567f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test1.png\n",
            "REF    : Dr. Mario Y. Maldana Griffith Miembro\n",
            "PRED   : Dr.MarioyMadanaGriffith\n",
            "CER    : 0.4054\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test2.png\n",
            "REF    : Abog. Carol Fernández Cáceres Actuaria Judicial\n",
            "PRED   : A bog. Carol Ferna g lez Caceres Act u arial udi cia l\n",
            "CER    : 0.2766\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test3.png\n",
            "REF    : Dr. AGUSTIN LOVERA CAÑETE MIEMBRO\n",
            "PRED   : DI. AGUST IN LOVER A CANE TE MIEM BRO\n",
            "CER    : 0.1818\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test4.png\n",
            "REF    : og. SANDRA FARÍAS de FERNÁNDEZ Juez Penal de Sentencia No. 3\n",
            "PRED   : FERNAN eZ og. S AND RAF ARIAS de en cia\n",
            "CER    : 0.7000\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test5.png\n",
            "REF    : Abg. Wilfrido Méndez Actuario Judicial\n",
            "PRED   : Abg. Wil fido Mer lez Act u a\n",
            "CER    : 0.4474\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test6.png\n",
            "REF    : ALMA MENDEZ DE BUONGERMINI Miembro Trib. Apel. Laboral\n",
            "PRED   : ALMA MEND EL DE SUO NGER MINI Mie m broT rib. Apel. Labor al\n",
            "CER    : 0.1852\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test7.png\n",
            "REF    : JULIO CÉSAR CENTENO B. Miembro\n",
            "PRED   : JULIO CESAR CENT ENO B Miembro\n",
            "CER    : 0.1000\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test8.png\n",
            "REF    : Eulalia Villalba Ojeda Asistente Estadística Penal - Asuncion\n",
            "PRED   : As is tente Es tadi stica Penal- Asuncion\n",
            "CER    : 0.4590\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test9.png\n",
            "REF    : ABG. LINA CASCO D. Actuaria Judicial\n",
            "PRED   : A BG. LINA CAS COD. Act u aria Judicial\n",
            "CER    : 0.1389\n",
            "================================================================================\n",
            "IMAGE  : /content/drive/MyDrive/OCR/Samples/Test10.png\n",
            "REF    : YOLANDA PORTILLO Juez Penal\n",
            "PRED   : X OL AND A PORTILLO Jue z Penal\n",
            "CER    : 0.1852\n",
            "\n",
            "################################################################################\n",
            "GLOBAL CER      : 0.3404\n",
            "EXACT MATCH RATE: 0.00%\n",
            "################################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "ft_inputs = processor(pil_image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    ft_ids = model.generate(\n",
        "        **ft_inputs,\n",
        "        do_sample=False,\n",
        "        num_beams=5,\n",
        "        tokenizer=processor.tokenizer,\n",
        "        stop_strings=\"<|im_end|>\",\n",
        "        max_new_tokens=24,\n",
        "        length_penalty=1,\n",
        "        # TODO: This is purely to prevent OCR: OCR: OCR: ... I may not even need it at this point\n",
        "        no_repeat_ngram_size=4,\n",
        "        repetition_penalty=1.1,\n",
        "    )\n",
        "\n",
        "ft_text = processor.decode(\n",
        "    ft_ids[0, ft_inputs[\"input_ids\"].shape[1]:],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "print(\"FINETUNED:\\n\" + ft_text)"
      ],
      "metadata": {
        "id": "HxL_9ANB-4w1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ea40b7-e3c1-4938-c095-efadb64ff849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINETUNED:\n",
            "Óg. SAVDRÁ FÁRIAS de FERNÁNREZ F. Pena! de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.disable_adapter()\n",
        "with torch.no_grad():\n",
        "    ids_no_lora = model.generate(\n",
        "        **ft_inputs,\n",
        "        do_sample=False,\n",
        "        tokenizer=processor.tokenizer,\n",
        "        stop_strings=\"<|im_end|>\",\n",
        "        max_new_tokens=64,\n",
        "    )\n",
        "\n",
        "txt_no_lora = processor.decode(\n",
        "    ids_no_lora[0, ft_inputs[\"input_ids\"].shape[1]:],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "print(\"NO LORA:\\n\" + txt_no_lora)\n"
      ],
      "metadata": {
        "id": "HpnWs5ZCBe_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "addb2026-de76-4b04-d508-429827b96c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NO LORA:\n",
            "FERNAN\n"
          ]
        }
      ]
    }
  ]
}